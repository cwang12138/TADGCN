CUDA: True cuda:0
folder_dir: ASTGCN_h2d1w2_channel1_1.000000e-03
params_path: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03
create params directory ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 2
nb_chev_filter	 64
nb_time_filter	 64
time_strides	 2
batch_size	 16
graph_signal_matrix_filename	 ./data/PEMS04/PEMS04.npz
start_epoch	 0
epochs	 100
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 2))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(30, 24, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1	torch.Size([307])
BlockList.0.TAt.U2	torch.Size([1, 307])
BlockList.0.TAt.U3	torch.Size([1])
BlockList.0.TAt.be	torch.Size([1, 60, 60])
BlockList.0.TAt.Ve	torch.Size([60, 60])
BlockList.0.SAt.W1	torch.Size([60])
BlockList.0.SAt.W2	torch.Size([1, 60])
BlockList.0.SAt.W3	torch.Size([1])
BlockList.0.SAt.bs	torch.Size([1, 307, 307])
BlockList.0.SAt.Vs	torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.Theta.0	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2	torch.Size([1, 64])
BlockList.0.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias	torch.Size([64])
BlockList.0.residual_conv.weight	torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias	torch.Size([64])
BlockList.0.ln.weight	torch.Size([64])
BlockList.0.ln.bias	torch.Size([64])
BlockList.1.TAt.U1	torch.Size([307])
BlockList.1.TAt.U2	torch.Size([64, 307])
BlockList.1.TAt.U3	torch.Size([64])
BlockList.1.TAt.be	torch.Size([1, 30, 30])
BlockList.1.TAt.Ve	torch.Size([30, 30])
BlockList.1.SAt.W1	torch.Size([30])
BlockList.1.SAt.W2	torch.Size([64, 30])
BlockList.1.SAt.W3	torch.Size([64])
BlockList.1.SAt.bs	torch.Size([1, 307, 307])
BlockList.1.SAt.Vs	torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.Theta.0	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2	torch.Size([64, 64])
BlockList.1.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias	torch.Size([64])
BlockList.1.residual_conv.weight	torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias	torch.Size([64])
BlockList.1.ln.weight	torch.Size([64])
BlockList.1.ln.bias	torch.Size([64])
final_conv.weight	torch.Size([24, 30, 1, 64])
final_conv.bias	torch.Size([24])
Net's total params: 496597
Optimizer's state_dict:
state	{}
param_groups	[{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_0.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_1.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_2.params
global step: 1000, training loss: 28.95, time: 250.60s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_3.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_4.params
global step: 2000, training loss: 31.66, time: 487.11s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_5.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_6.params
global step: 3000, training loss: 22.99, time: 723.88s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_7.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_8.params
global step: 4000, training loss: 21.20, time: 962.29s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_10.params
global step: 5000, training loss: 26.21, time: 1199.32s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_12.params
global step: 6000, training loss: 22.24, time: 1436.87s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_13.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_14.params
global step: 7000, training loss: 25.06, time: 1675.08s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_15.params
global step: 8000, training loss: 25.69, time: 1912.56s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_17.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_18.params
global step: 9000, training loss: 21.51, time: 2150.38s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_19.params
global step: 10000, training loss: 23.29, time: 2388.75s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_22.params
global step: 11000, training loss: 24.30, time: 2627.34s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_23.params
global step: 12000, training loss: 19.98, time: 2866.01s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_25.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_26.params
global step: 13000, training loss: 19.28, time: 3104.73s
global step: 14000, training loss: 25.48, time: 3342.64s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_29.params
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_30.params
global step: 15000, training loss: 20.12, time: 3580.85s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_31.params
global step: 16000, training loss: 22.57, time: 3819.42s
global step: 17000, training loss: 22.30, time: 4058.20s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_35.params
global step: 18000, training loss: 22.56, time: 4296.89s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_39.params
global step: 19000, training loss: 22.16, time: 4535.24s
global step: 20000, training loss: 19.05, time: 4773.06s
global step: 21000, training loss: 20.26, time: 5011.37s
global step: 22000, training loss: 19.83, time: 5250.11s
global step: 23000, training loss: 22.75, time: 5488.92s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_48.params
global step: 24000, training loss: 19.93, time: 5727.63s
global step: 25000, training loss: 20.16, time: 5965.54s
global step: 26000, training loss: 20.44, time: 6203.55s
global step: 27000, training loss: 22.46, time: 6442.07s
global step: 28000, training loss: 23.80, time: 6680.71s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_58.params
global step: 29000, training loss: 19.16, time: 6919.54s
global step: 30000, training loss: 21.00, time: 7158.19s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_63.params
global step: 31000, training loss: 24.20, time: 7396.07s
global step: 32000, training loss: 21.25, time: 7634.54s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_67.params
global step: 33000, training loss: 19.03, time: 7873.18s
global step: 34000, training loss: 23.23, time: 8111.96s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_72.params
global step: 35000, training loss: 21.15, time: 8350.67s
global step: 36000, training loss: 18.96, time: 8589.02s
global step: 37000, training loss: 22.20, time: 8826.95s
global step: 38000, training loss: 20.70, time: 9065.18s
global step: 39000, training loss: 21.28, time: 9303.95s
global step: 40000, training loss: 20.49, time: 9542.61s
global step: 41000, training loss: 22.77, time: 9781.47s
global step: 42000, training loss: 18.89, time: 10019.34s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_87.params
global step: 43000, training loss: 20.78, time: 10257.19s
global step: 44000, training loss: 20.85, time: 10495.54s
global step: 45000, training loss: 18.91, time: 10734.12s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_93.params
global step: 46000, training loss: 20.64, time: 10972.89s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_95.params
global step: 47000, training loss: 22.17, time: 11211.42s
save parameters to file: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_98.params
global step: 48000, training loss: 17.76, time: 11449.21s
best epoch: 98
load weight from: ./experiments/PEMS04/2023-08-22-22-42-03\ASTGCN_h2d1w2_channel1_1.000000e-03\epoch_98.params
predicting data set batch 1 / 162
predicting data set batch 101 / 162
input: (2588, 307, 1, 60)
prediction: (2588, 307, 24)
data_target_tensor: (2588, 307, 24)
current epoch: 98, predict 0 points
MAE: 18.04
RMSE: 28.87
MAPE: 0.13
current epoch: 98, predict 1 points
MAE: 18.96
RMSE: 30.33
MAPE: 0.13
current epoch: 98, predict 2 points
MAE: 19.51
RMSE: 31.23
MAPE: 0.14
current epoch: 98, predict 3 points
MAE: 19.96
RMSE: 31.98
MAPE: 0.14
current epoch: 98, predict 4 points
MAE: 20.22
RMSE: 32.46
MAPE: 0.14
current epoch: 98, predict 5 points
MAE: 20.60
RMSE: 33.03
MAPE: 0.14
current epoch: 98, predict 6 points
MAE: 20.80
RMSE: 33.44
MAPE: 0.14
current epoch: 98, predict 7 points
MAE: 21.09
RMSE: 33.88
MAPE: 0.15
current epoch: 98, predict 8 points
MAE: 21.13
RMSE: 34.10
MAPE: 0.14
current epoch: 98, predict 9 points
MAE: 21.34
RMSE: 34.42
MAPE: 0.15
current epoch: 98, predict 10 points
MAE: 21.53
RMSE: 34.71
MAPE: 0.15
current epoch: 98, predict 11 points
MAE: 22.01
RMSE: 35.36
MAPE: 0.15
current epoch: 98, predict 12 points
MAE: 22.75
RMSE: 36.33
MAPE: 0.16
current epoch: 98, predict 13 points
MAE: 23.42
RMSE: 37.30
MAPE: 0.16
current epoch: 98, predict 14 points
MAE: 23.94
RMSE: 38.07
MAPE: 0.16
current epoch: 98, predict 15 points
MAE: 24.35
RMSE: 38.68
MAPE: 0.17
current epoch: 98, predict 16 points
MAE: 24.75
RMSE: 39.25
MAPE: 0.17
current epoch: 98, predict 17 points
MAE: 25.16
RMSE: 39.86
MAPE: 0.18
current epoch: 98, predict 18 points
MAE: 25.70
RMSE: 40.61
MAPE: 0.18
current epoch: 98, predict 19 points
MAE: 26.26
RMSE: 41.39
MAPE: 0.19
current epoch: 98, predict 20 points
MAE: 26.81
RMSE: 42.15
MAPE: 0.19
current epoch: 98, predict 21 points
MAE: 27.38
RMSE: 42.88
MAPE: 0.20
current epoch: 98, predict 22 points
MAE: 28.09
RMSE: 43.77
MAPE: 0.20
current epoch: 98, predict 23 points
MAE: 29.08
RMSE: 45.00
MAPE: 0.21
all MAE: 23.04
all RMSE: 36.89
all MAPE: 0.16
[18.044449, 28.868100882031644, 0.12992716, 18.962309, 30.326020864991605, 0.13497429, 19.512024, 31.23386204399952, 0.13778308, 19.95742, 31.98421088746368, 0.1398135, 20.221693, 32.461243041582485, 0.14124557, 20.59692, 33.026657896855646, 0.14420153, 20.797226, 33.43771904133863, 0.14355247, 21.092402, 33.883364612460454, 0.14550082, 21.132465, 34.0983648629685, 0.14380723, 21.338215, 34.4220274905837, 0.14545225, 21.534645, 34.70905685739224, 0.1467408, 22.00621, 35.36239561225314, 0.15036176, 22.754602, 36.33029227406174, 0.15562323, 23.42207, 37.300314697901676, 0.15972145, 23.942017, 38.068896027157955, 0.16430505, 24.348091, 38.67808504431563, 0.16821426, 24.747828, 39.249889592300924, 0.17218946, 25.156984, 39.85889418075343, 0.17643403, 25.703882, 40.60628024231212, 0.18125543, 26.260204, 41.39339128166325, 0.18666996, 26.812487, 42.145591163159494, 0.19065878, 27.384438, 42.877619271305164, 0.19634487, 28.085367, 43.772266933973874, 0.20181465, 29.07922, 45.00431836918542, 0.21325316, 23.037233, 36.894665042438724, 0.1612438]
test time: 21 seconds.
Horizon 1: mae: 18.07, rmse: 28.64, mape: 12.99%.
Horizon 2: mae: 18.97, rmse: 30.02, mape: 13.50%.
Horizon 3: mae: 19.50, rmse: 30.87, mape: 13.78%.
Horizon 4: mae: 19.93, rmse: 31.56, mape: 13.98%.
Horizon 5: mae: 20.18, rmse: 32.00, mape: 14.12%.
Horizon 6: mae: 20.55, rmse: 32.52, mape: 14.42%.
Horizon 7: mae: 20.74, rmse: 32.89, mape: 14.36%.
Horizon 8: mae: 21.03, rmse: 33.28, mape: 14.55%.
Horizon 9: mae: 21.06, rmse: 33.47, mape: 14.38%.
Horizon 10: mae: 21.26, rmse: 33.75, mape: 14.55%.
Horizon 11: mae: 21.45, rmse: 34.00, mape: 14.67%.
Horizon 12: mae: 21.91, rmse: 34.59, mape: 15.04%.
Horizon 13: mae: 22.64, rmse: 35.49, mape: 15.56%.
Horizon 14: mae: 23.29, rmse: 36.37, mape: 15.97%.
Horizon 15: mae: 23.80, rmse: 37.08, mape: 16.43%.
Horizon 16: mae: 24.20, rmse: 37.63, mape: 16.82%.
Horizon 17: mae: 24.59, rmse: 38.17, mape: 17.22%.
Horizon 18: mae: 24.99, rmse: 38.75, mape: 17.64%.
Horizon 19: mae: 25.53, rmse: 39.46, mape: 18.13%.
Horizon 20: mae: 26.08, rmse: 40.22, mape: 18.67%.
Horizon 21: mae: 26.63, rmse: 40.96, mape: 19.07%.
Horizon 22: mae: 27.20, rmse: 41.69, mape: 19.63%.
Horizon 23: mae: 27.90, rmse: 42.58, mape: 20.18%.
Horizon 24: mae: 28.88, rmse: 43.79, mape: 21.33%.
Average Error: mae: 22.93, rmse: 35.82, mape: 16.12%.
